{
  
    
        "post0": {
            "title": "Visualising text data with unsupervised learning",
            "content": "Intro . Visualisation is an essential part of working with data. Whether we are delivering regular reports to decision-makers, presenting the results of a regression model to a room of stakeholders, or creating a real-time dashboard for the wider business, we are using data to tell a story, and visuals bring these stories to life. . In this post we are going to explore a number of ways to visualise text data in the context of analysing job posts for three data roles: data scientists, data engineers and data analysts. These roles have a lot in common but are fundamentally different; will the content of the job descriptions for these roles reflect the differences between them? And how can we visualise these differences? . Getting the data . First we need to collect some job descriptions to analyse. There are plenty of job search APIs out there to choose from and we also have the option to scrape directly from job websites. I ended up using the Reed developer API because of the simple process of signing up for credentials. I wrote a python wrapper for the API which I will be using to extract a collection of relevant job posts; you can find it here. . It&#39;s a simple process to fetch the jobs matching a keywords search query. The response is a list of json objects which in this case are identical to python dictionaries so we can easily place the data in a pandas dataframe. . import pandas as pd from reed import ReedClient client = ReedClient(api_key=api_key) search_params = { &#39;keywords&#39;: &#39;data+scientist|data+engineer|data+analyst&#39;, &#39;resultsToTake&#39;: 600 } response = client.search(**search_params) df = pd.DataFrame(response) df.shape . (524, 15) . Our search returned 524 jobs in total; let&#39;s clean up the job titles and see what we have. . def clean_title(title_str): return title_str.lower().strip().replace(&#39; &#39;, &#39; &#39;) df[&#39;jobTitle&#39;] = [clean_title(x) for x in df.jobTitle] df.groupby(&#39;jobTitle&#39;).size().sort_values(ascending=False).head(10) . jobTitle data analyst 62 data engineer 46 data scientist 43 senior data scientist 19 senior data engineer 11 big data engineer 5 lead data scientist 5 lead data engineer 4 data scientist - manchester 4 graduate data analyst 4 dtype: int64 . We&#39;re going to remove all the posts which don&#39;t have one of the three most common titles; what we&#39;re left with should be broadly similar in terms of representing mid-level roles within each category. . accepted_titles = [&#39;data scientist&#39;, &#39;data engineer&#39;, &#39;data analyst&#39;] df = df[[x in accepted_titles for x in df.jobTitle]].set_index(&#39;jobId&#39;) . The job search API gives a truncated version of the job descriptions so if we want the complete text we&#39;ll have to take each individual job id and pull down the details using the job details function one-by-one. To make this process easier I created a dictionary with the three job titles as keys and the items consisting of the list of ids corresponding to each title. . groups = df.groupby(&#39;jobTitle&#39;).groups groups.keys() . dict_keys([&#39;data analyst&#39;, &#39;data engineer&#39;, &#39;data scientist&#39;]) . Now we can loop over the dictionary pulling down the description from each job in turn. There&#39;s another complication in that the job descriptions are returned as html documents. We&#39;re only interested in the text data so we&#39;re going to have the parse the html to extract the information we want. We can wrap this process inside a function which we call for each id in our dictionary. . def get_job_desc(job_type, job_id, client, html_parser): desc_html = client.job_details(job_id)[&#39;jobDescription&#39;] parser.feed(desc_html) desc_str = parser.return_data() # reset parser string parser.string = &#39;&#39; return dict(job=job_type, job_id=job_id, desc=desc_str) parser = MyHTMLParser() job_descriptions = [] for title in groups: for id in groups[title]: job_desc_dict = get_job_desc(title, id, client, parser) job_descriptions.append(job_desc_dict) df = pd.DataFrame(job_descriptions) df.head() . job job_id desc . 0 data analyst | 40501878 | Data Analyst | Healthcare / MedTech | SQL, Tab... | . 1 data analyst | 40532763 | Data Analyst £35,000 - £40,000 + Benefits Lo... | . 2 data analyst | 40500203 | A well established Telecommunications company ... | . 3 data analyst | 40500186 | A well established Telecommunications company ... | . 4 data analyst | 40531962 | My client, a multinational, speciality insuran... | . In order to visualise the content of the job descriptions we need a numerical representation of the text. One way to do this is with a bag-of-words approach, which consists of separating the text in our documents into tokens and counting the appearances of each token in each document. Before we do this there are some errands we need to run first. . Sklearn&#39;s Countvectorizer class performs the tokenization step for us, but we need to be careful when using it. By default it splits the text on punctuation marks and discards tokens less than two characters long. For this reason, the word &#39;Ph.D&#39; which is frequently used would be split into the two meaningless tokens &#39;Ph&#39; and &#39;D&#39;, for example. The important term &#39;R&#39; denoting the programming language would also be discarded. We can remedy these issues and others similar using pandas string methods. At the same time we&#39;ll remove any rows with duplicate job descriptions. . df = df[~df.duplicated(&#39;desc&#39;)] cicd_pat = &quot;([Cc][Ii]/[Cc][Dd])&quot; phd_pat = &quot;[Pp][Hh].?[Dd]&quot; r_pat = &#39;( sR W)&#39; df[&#39;desc&#39;] = (df.desc.str.replace(&#39;,000&#39;, &#39;k&#39;) .str.replace(&#39; xa0&#39;, &#39; &#39;) .str.replace(phd_pat, &#39;PHD&#39;) .str.replace(r_pat, &#39; RStudio &#39;) .str.replace(cicd_pat, &#39; CICD&#39;) .str.replace(&#39;Modis&#39;, &#39;&#39;)) . Now we&#39;re finally at a stage where our data is ready to be analysed. After whittling the results down to mid-level roles we were left with 133 unique jobs with a roughly even split between the three roles. . When we fit the CountVectorizer transform to an array containing each job description what we get is a document-term matrix: the rows are the job descriptions and the columns are all the words which were found in the collection of documents. The contents of the matrix are the word frequencies. . from sklearn.feature_extraction.text import CountVectorizer count_vectorizer = CountVectorizer(stop_words=&#39;english&#39;) text_data = np.array(df[&#39;desc&#39;]) count_matrix = count_vectorizer.fit_transform(text_data) count_matrix_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names()) count_matrix_df . 00 000 025 04 06 075 08 10 100 100k 1070906 11 12 13 1376154 14 15 16 17 18 184 19 1997 1x 20 200 2003 2012 2018 2019 2020 20th 210 22nd 23 24 240 24k 24x7 25 ... won word words work worked workers workflow workflows workforce working workload workloads workplace works workshops world worldwide worry worth wrangling wrexham wrigley write writers writing written wrong wtl ww www x4 xml yagni yarn year years yields yoga yorkshire zachman . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 128 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 0 | 1 | 4 | 1 | 1 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 129 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 130 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 131 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 132 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 133 rows × 3741 columns . How can we use this implementation to visualise the three different roles? A simple option is to look at the most frequent words across the three classes: if we group the documents into the three roles, sum the word frequencies and sort them in descending order we&#39;ll see which words appear the most in each. . . . We can spot a few themes such as the emphasis on machine learning in DS job descriptions while SQL and reporting feature prominently in DA posts. The plot corresponding to Data engineer roles is not as insightful. The appearance of Python, SQL and Azure are promising, but no sign of ETL pipelines and a lower prevalance of cloud-based systems than we&#39;d expect. And why does scientist turn up so much? Across the three plots we can also see another problem with this simple word frequency approach: the most insightful terms are diluted by a saturation of words which are common across the three roles and thus essentially useless, such as &#39;data&#39;, &#39;experience&#39;, &#39;role&#39; and &#39;team&#39;. . We need a metric which highly ranks frequent words but suppresses words which appear in lots of the documents in our corpus. Luckily for us a metric exists which does exactly as we require. TFIDF or term-frequency inverse-document frequency takes the term frequency of a word in a document and multiplies it by the inverse document freqency of that word, essentially damping those words which appear across a large proportion of the documents. Again sklearn has our back and can perform this process for us. . from sklearn.feature_extraction.text import TfidfVectorizer max_df = 0.6 min_df = 2 vectorizer = TfidfVectorizer(stop_words=&#39;english&#39;, max_df=max_df, min_df=min_df) text = np.array(df[&#39;desc&#39;]) tfidf_matrix = vectorizer.fit_transform(text) tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names()) tfidf . 00 08 10 100 1070906 12 14 16 17 19 1997 20 2003 2018 2019 2020 20th 240 24k 25 25k 26 28 30 300 30k 31k 32k 35k 40 400 40k 450 45k 500 50k 55k 60 60k 65k ... warrington watch watching way ways web website week weeks welcome welcomes wellbeing west whilst wide wider willing willingness winning won word worked workers workflow workflows workforce workload workloads workplace works world worldwide wrangling wrexham write writing written x4 year years . 0 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.151358 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.00000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | . 1 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.168567 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.00000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.126809 | 0.0 | 0.086106 | 0.0 | 0.0 | 0.0 | 0.000000 | . 2 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.135622 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.00000 | 0.257828 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | . 3 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.168772 | 0.096335 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.07677 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.071167 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.102478 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | . 4 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.196656 | 0.0 | 0.105206 | 0.0 | 0.000000 | 0.110827 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.00000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.102739 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.100455 | 0.0 | 0.0 | 0.0 | 0.000000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 128 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.059399 | 0.0 | 0.000000 | 0.0 | 0.062064 | 0.000000 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.0 | 0.0 | 0.054997 | 0.0 | 0.0 | 0.0 | 0.00000 | 0.079857 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.076462 | 0.0 | 0.073592 | 0.060684 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.068912 | 0.053134 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.044717 | . 129 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.069856 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | ... | 0.084834 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.00000 | 0.075804 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.061867 | 0.0 | 0.0 | 0.063552 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | . 130 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.00000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.093065 | . 131 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.00000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.000000 | 0.125361 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | . 132 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.079108 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.0 | 0.0 | 0.054481 | 0.0 | 0.0 | 0.0 | 0.00000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.068265 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.052635 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.044297 | . 133 rows × 2038 columns . As before the TfidfVectorizer returns a term-document matrix, but instead of consisting of word frequencies we have tfidf values for each term-document pair. The parameters &#39;min_df&#39; and &#39;max_df&#39; are constraints on the document frequency of the words in our vocabulary. We have used a &#39;min_df&#39; of 2, so any words that do not appear in at least two documents are thrown out. &#39;Max_df&#39; is a float which corresponds to a proportion of the documents, so here if any words appear in more than 60 percent of the documents they too are discarded. In the same way as before, we can sum the tfidf values within the three classes and plot the highest aggregate values to hopefully extract the most important terms to characterise the three job types. . . . This is a big improvement over our first attempt. There are far fewer generic terms because of the introduction of the inverse-document weighting and the document frequency constraints. The data engineering terms are also closer to our expectations than they were before. . What are the key insights we can take from these plots? For data scientists machine learning is the most distinguishing feature. R and Python both appear with the latter ranked slightly higher, at least in this sample. We can also see that doctorates are important in data scientist posts but not in the other two roles. . Our picture of data engineering posts is clearer from this version of the plot. Cloud technologies, ETL pipelines and databases feature heavily and python seems to be the scripting language of choice within this sample of posts. The impression given by this visual is that tools and technologies are to data engineering what algorithms and models are to data science. . Finally, data analyst positions appear to be characterised more by aptitudes and skills than by specific technologies or technical backgrounds. SQL and excel feature heavily but apart from these tools the key terms seem to describe an general analytical mindset and an ability to support other business functions through reporting and analysis. . Plotting job descriptions via dimensionality reduction . So far we&#39;ve used the TF-IDF vectors to gain insight into the key words that describe the different data roles. We haven&#39;t, however, used any unsupervised learning as we promised at the beginning. Let&#39;s sort that out now. . Using TF-IDF we have created a 2038-dimensional representation of the job descriptions in our sample. As it is a bit of a struggle to visualise anything in more than three dimensions, it is common practice to manipulate high-dimensional data into a two or three dimensional space so that it can be visualised and structural patterns in the data can be more easily found. In this case we are going to use Principal Component Analysis, or PCA for short, to project our data onto the two dimensions which retain the highest variability. This will allow us to create a scatterplot of the data. . from sklearn.decomposition import PCA pca = PCA(n_components=2) X = pca.fit_transform(tfidf) X_df = pd.DataFrame(X) X_df[&#39;job&#39;] = df.job.values X_df = X_df.rename(columns={0: &#39;pc_0&#39;, 1: &#39;pc_1&#39;}) X_df.sample(5) . pc_0 pc_1 job . 102 0.210452 | -0.119038 | data scientist | . 6 -0.135975 | -0.110068 | data analyst | . 55 0.111871 | 0.328581 | data engineer | . 81 -0.068974 | 0.126661 | data engineer | . 31 -0.205418 | -0.054962 | data analyst | . pca.explained_variance_ . array([0.03001154, 0.02439249]) . Using only a two dimensional representation of the TF-IDF vectors we retain just over half of the variance in our data. The following is a scatterplot of the first two principle components; each point is a job description. . import plotly.express as px fig = px.scatter(X_df, x=&quot;pc_0&quot;, y=&quot;pc_1&quot;, color=&quot;job&quot;, hover_data=[&quot;pc_0&quot;, &quot;pc_1&quot;, X_df.index]) fig.update_layout(title=&#39;first two principle components of tfidf matrix&#39;) fig.show() . . . Remarkably, the three different job titles are almost perfectly separated into clusters in this two dimensional representation. This is significant because the PCA algorithm has no idea that our dataset contains three distinct classes, but by simply projecting the data onto the coordinate axes retaining the most variability the algorithm has managed to almost perfectly separate them. We can go one step further and demonstrate that the three classes are well-separated by applying k-means clustering first two principle components. The result is the following. . from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=3) # X is the data matrix of the first two PC&#39;s kmeans.fit(X) y_kmeans = kmeans.predict(X) centers = kmeans.cluster_centers_ . . . Let&#39;s recap what we&#39;ve achieved here. We started with a collection of raw text documents, each one a description of a job from one of three different titles. Using term-frequency inverse-document-freqency we converted each description into a fixed length vector of TF-IDF values. Then using PCA we projected our new 2038-dimensional data matrix onto the two coordinate axes of highest variability so we could produce a scatterplot of the data. The result was a clear visual separation into three clusters corresponding to the three job titles, which we have now shown are nearly perfectly recreated using a simple clustering algorithm. . The cool part about this is not that we have accurately classified each TF-IDF vector with its job title. There are lots of supervised approaches that would likely be more effective if that were our goal. No, the interesting part is thinking about how these simple unsupervised methods can be applied to problems where we don&#39;t have access to labelled data, as we did here. For example, given an unlabelled text dataset we could take the same steps, compute the TF-IDF values, plot the first two principle components and look for clusters. Then by comparing the most important words in each cluster via the TF-IDF values we might uncover patterns in the data that we weren&#39;t aware of. What clusters might we find if we applied this process to restaurant reviews? Or house listings? . Hopefully some of these ideas for visualising and investigating text data have piqued your interest and you&#39;ve learned some of the differences between data science, engineering and analysis roles along the way. For further reading, Jake VanderPlas has released great content for free covering TF-IDF statistics, PCA and clustering among other topics. .",
            "url": "https://theo-r.github.io/datablog/python/tfidf/pca/clustering/plotly/nlp/2020/06/29/Visualising-text-data-with-unsupervised-learning.html",
            "relUrl": "/python/tfidf/pca/clustering/plotly/nlp/2020/06/29/Visualising-text-data-with-unsupervised-learning.html",
            "date": " • Jun 29, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "",
          "url": "https://theo-r.github.io/datablog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://theo-r.github.io/datablog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}